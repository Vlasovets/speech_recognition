{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overfitting.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vlasovets/speech_recognition/blob/master/Overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niyFb708H_Fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyFcMqvVcHJ5",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVH_mMFraiYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('csv_train.csv', sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXT5hU0Uaiig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "header = np.arange(0, 6374)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu6eeyWGairD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.columns = header"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Httec4ZYaive",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "9819c2dd-8f07-4b60-fc42-f0d3b9a52272"
      },
      "source": [
        "train.head(2)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>6334</th>\n",
              "      <th>6335</th>\n",
              "      <th>6336</th>\n",
              "      <th>6337</th>\n",
              "      <th>6338</th>\n",
              "      <th>6339</th>\n",
              "      <th>6340</th>\n",
              "      <th>6341</th>\n",
              "      <th>6342</th>\n",
              "      <th>6343</th>\n",
              "      <th>6344</th>\n",
              "      <th>6345</th>\n",
              "      <th>6346</th>\n",
              "      <th>6347</th>\n",
              "      <th>6348</th>\n",
              "      <th>6349</th>\n",
              "      <th>6350</th>\n",
              "      <th>6351</th>\n",
              "      <th>6352</th>\n",
              "      <th>6353</th>\n",
              "      <th>6354</th>\n",
              "      <th>6355</th>\n",
              "      <th>6356</th>\n",
              "      <th>6357</th>\n",
              "      <th>6358</th>\n",
              "      <th>6359</th>\n",
              "      <th>6360</th>\n",
              "      <th>6361</th>\n",
              "      <th>6362</th>\n",
              "      <th>6363</th>\n",
              "      <th>6364</th>\n",
              "      <th>6365</th>\n",
              "      <th>6366</th>\n",
              "      <th>6367</th>\n",
              "      <th>6368</th>\n",
              "      <th>6369</th>\n",
              "      <th>6370</th>\n",
              "      <th>6371</th>\n",
              "      <th>6372</th>\n",
              "      <th>6373</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.554157</td>\n",
              "      <td>0.134117</td>\n",
              "      <td>0.982365</td>\n",
              "      <td>0.399174</td>\n",
              "      <td>0.74399</td>\n",
              "      <td>1.083199</td>\n",
              "      <td>0.344815</td>\n",
              "      <td>0.339210</td>\n",
              "      <td>0.684025</td>\n",
              "      <td>0.125897</td>\n",
              "      <td>1.965657</td>\n",
              "      <td>1.839759</td>\n",
              "      <td>0.454876</td>\n",
              "      <td>0.596813</td>\n",
              "      <td>2.818323</td>\n",
              "      <td>0.596389</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.195990</td>\n",
              "      <td>0.497279</td>\n",
              "      <td>0.111038</td>\n",
              "      <td>0.006749</td>\n",
              "      <td>0.001089</td>\n",
              "      <td>0.442727</td>\n",
              "      <td>0.516227</td>\n",
              "      <td>0.006580</td>\n",
              "      <td>-1.885221</td>\n",
              "      <td>0.909765</td>\n",
              "      <td>0.550789</td>\n",
              "      <td>-0.898141</td>\n",
              "      <td>0.334471</td>\n",
              "      <td>5.576216</td>\n",
              "      <td>0.134553</td>\n",
              "      <td>0.690398</td>\n",
              "      <td>0.784789</td>\n",
              "      <td>1.065601</td>\n",
              "      <td>1.441291</td>\n",
              "      <td>0.280812</td>\n",
              "      <td>0.375691</td>\n",
              "      <td>0.656502</td>\n",
              "      <td>...</td>\n",
              "      <td>0.607063</td>\n",
              "      <td>3.029264</td>\n",
              "      <td>3.027681</td>\n",
              "      <td>1913.0150</td>\n",
              "      <td>0.568106</td>\n",
              "      <td>130.5027</td>\n",
              "      <td>61.86271</td>\n",
              "      <td>133.7167</td>\n",
              "      <td>68.91190</td>\n",
              "      <td>0.667082</td>\n",
              "      <td>1.723316</td>\n",
              "      <td>2.167282</td>\n",
              "      <td>0.100993</td>\n",
              "      <td>0.057842</td>\n",
              "      <td>10.273500</td>\n",
              "      <td>0.630777</td>\n",
              "      <td>2.691292</td>\n",
              "      <td>2.691582</td>\n",
              "      <td>-9273.994</td>\n",
              "      <td>0.646643</td>\n",
              "      <td>123.9579</td>\n",
              "      <td>61.94287</td>\n",
              "      <td>119.4142</td>\n",
              "      <td>58.81306</td>\n",
              "      <td>0.651420</td>\n",
              "      <td>1.403322</td>\n",
              "      <td>1.849628</td>\n",
              "      <td>0.103968</td>\n",
              "      <td>0.059069</td>\n",
              "      <td>8.239951</td>\n",
              "      <td>0.547856</td>\n",
              "      <td>2.296115</td>\n",
              "      <td>2.294919</td>\n",
              "      <td>1920.215</td>\n",
              "      <td>0.619489</td>\n",
              "      <td>105.91370</td>\n",
              "      <td>54.43164</td>\n",
              "      <td>104.58150</td>\n",
              "      <td>52.95197</td>\n",
              "      <td>FRE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.524493</td>\n",
              "      <td>0.183608</td>\n",
              "      <td>0.001498</td>\n",
              "      <td>0.189475</td>\n",
              "      <td>0.22996</td>\n",
              "      <td>0.342812</td>\n",
              "      <td>0.040485</td>\n",
              "      <td>0.112852</td>\n",
              "      <td>0.153337</td>\n",
              "      <td>0.108781</td>\n",
              "      <td>1.170879</td>\n",
              "      <td>1.062097</td>\n",
              "      <td>0.221155</td>\n",
              "      <td>2.352384</td>\n",
              "      <td>9.032930</td>\n",
              "      <td>1.100833</td>\n",
              "      <td>11.44</td>\n",
              "      <td>0.46</td>\n",
              "      <td>1.879602</td>\n",
              "      <td>0.171196</td>\n",
              "      <td>0.053285</td>\n",
              "      <td>0.008132</td>\n",
              "      <td>0.003424</td>\n",
              "      <td>0.489084</td>\n",
              "      <td>0.503961</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>-1.924282</td>\n",
              "      <td>1.108105</td>\n",
              "      <td>0.236835</td>\n",
              "      <td>-0.703318</td>\n",
              "      <td>0.292012</td>\n",
              "      <td>3.307423</td>\n",
              "      <td>0.131607</td>\n",
              "      <td>0.365504</td>\n",
              "      <td>0.890798</td>\n",
              "      <td>0.987849</td>\n",
              "      <td>1.142710</td>\n",
              "      <td>0.097051</td>\n",
              "      <td>0.154861</td>\n",
              "      <td>0.251912</td>\n",
              "      <td>...</td>\n",
              "      <td>0.565753</td>\n",
              "      <td>2.419919</td>\n",
              "      <td>2.417481</td>\n",
              "      <td>992.7057</td>\n",
              "      <td>0.589380</td>\n",
              "      <td>114.4943</td>\n",
              "      <td>60.13251</td>\n",
              "      <td>117.3509</td>\n",
              "      <td>59.54166</td>\n",
              "      <td>0.653157</td>\n",
              "      <td>1.327391</td>\n",
              "      <td>1.657223</td>\n",
              "      <td>0.089386</td>\n",
              "      <td>0.049955</td>\n",
              "      <td>6.988263</td>\n",
              "      <td>0.589441</td>\n",
              "      <td>2.034590</td>\n",
              "      <td>2.033893</td>\n",
              "      <td>2917.176</td>\n",
              "      <td>0.567629</td>\n",
              "      <td>103.5912</td>\n",
              "      <td>49.19407</td>\n",
              "      <td>101.1961</td>\n",
              "      <td>48.73297</td>\n",
              "      <td>0.657664</td>\n",
              "      <td>1.182425</td>\n",
              "      <td>1.498260</td>\n",
              "      <td>0.094776</td>\n",
              "      <td>0.053675</td>\n",
              "      <td>8.251408</td>\n",
              "      <td>0.607847</td>\n",
              "      <td>1.906892</td>\n",
              "      <td>1.906919</td>\n",
              "      <td>-71227.270</td>\n",
              "      <td>0.463597</td>\n",
              "      <td>94.94153</td>\n",
              "      <td>46.28285</td>\n",
              "      <td>93.48928</td>\n",
              "      <td>45.68927</td>\n",
              "      <td>SPA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 6374 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0         1         2         3     ...      6370       6371      6372  6373\n",
              "0  2.554157  0.134117  0.982365  0.399174  ...  54.43164  104.58150  52.95197   FRE\n",
              "1  1.524493  0.183608  0.001498  0.189475  ...  46.28285   93.48928  45.68927   SPA\n",
              "\n",
              "[2 rows x 6374 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odhj7egjaioy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "194d2c96-2737-4dcc-f9d2-23741cb35f4d"
      },
      "source": [
        "y_train = train[6373]\n",
        "y_train.head(2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    FRE\n",
              "1    SPA\n",
              "Name: 6373, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn1M3qw0aimb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train.iloc[:, :-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z8dIdLvcFcy",
        "colab_type": "text"
      },
      "source": [
        "## Dev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDJC8W0Aa4pG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev = pd.read_csv('csv_dev.csv', sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SWEGS4da4xj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev.columns = header"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMdoM49na4vo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "863f63c3-79fc-41c2-f4af-61dfa4b9ecda"
      },
      "source": [
        "dev.head(2)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>6334</th>\n",
              "      <th>6335</th>\n",
              "      <th>6336</th>\n",
              "      <th>6337</th>\n",
              "      <th>6338</th>\n",
              "      <th>6339</th>\n",
              "      <th>6340</th>\n",
              "      <th>6341</th>\n",
              "      <th>6342</th>\n",
              "      <th>6343</th>\n",
              "      <th>6344</th>\n",
              "      <th>6345</th>\n",
              "      <th>6346</th>\n",
              "      <th>6347</th>\n",
              "      <th>6348</th>\n",
              "      <th>6349</th>\n",
              "      <th>6350</th>\n",
              "      <th>6351</th>\n",
              "      <th>6352</th>\n",
              "      <th>6353</th>\n",
              "      <th>6354</th>\n",
              "      <th>6355</th>\n",
              "      <th>6356</th>\n",
              "      <th>6357</th>\n",
              "      <th>6358</th>\n",
              "      <th>6359</th>\n",
              "      <th>6360</th>\n",
              "      <th>6361</th>\n",
              "      <th>6362</th>\n",
              "      <th>6363</th>\n",
              "      <th>6364</th>\n",
              "      <th>6365</th>\n",
              "      <th>6366</th>\n",
              "      <th>6367</th>\n",
              "      <th>6368</th>\n",
              "      <th>6369</th>\n",
              "      <th>6370</th>\n",
              "      <th>6371</th>\n",
              "      <th>6372</th>\n",
              "      <th>6373</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.530088</td>\n",
              "      <td>0.635352</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.641956</td>\n",
              "      <td>1.013997</td>\n",
              "      <td>1.676991</td>\n",
              "      <td>0.37204</td>\n",
              "      <td>0.662994</td>\n",
              "      <td>1.035035</td>\n",
              "      <td>0.506228</td>\n",
              "      <td>2.873326</td>\n",
              "      <td>2.367098</td>\n",
              "      <td>0.624598</td>\n",
              "      <td>0.837602</td>\n",
              "      <td>2.877323</td>\n",
              "      <td>0.661884</td>\n",
              "      <td>6.40</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.706188</td>\n",
              "      <td>0.490691</td>\n",
              "      <td>0.162208</td>\n",
              "      <td>0.013482</td>\n",
              "      <td>0.001926</td>\n",
              "      <td>0.469178</td>\n",
              "      <td>0.527296</td>\n",
              "      <td>0.007687</td>\n",
              "      <td>-2.050633</td>\n",
              "      <td>1.272256</td>\n",
              "      <td>0.200493</td>\n",
              "      <td>-0.693913</td>\n",
              "      <td>0.278973</td>\n",
              "      <td>3.002646</td>\n",
              "      <td>0.249732</td>\n",
              "      <td>0.638776</td>\n",
              "      <td>0.834926</td>\n",
              "      <td>1.009232</td>\n",
              "      <td>1.306881</td>\n",
              "      <td>0.174305</td>\n",
              "      <td>0.297649</td>\n",
              "      <td>0.471954</td>\n",
              "      <td>...</td>\n",
              "      <td>0.588955</td>\n",
              "      <td>3.062866</td>\n",
              "      <td>3.060469</td>\n",
              "      <td>1277.318</td>\n",
              "      <td>0.583678</td>\n",
              "      <td>128.8150</td>\n",
              "      <td>67.51817</td>\n",
              "      <td>128.1010</td>\n",
              "      <td>63.50588</td>\n",
              "      <td>0.664635</td>\n",
              "      <td>1.627804</td>\n",
              "      <td>2.053371</td>\n",
              "      <td>0.096522</td>\n",
              "      <td>0.054848</td>\n",
              "      <td>8.578906</td>\n",
              "      <td>0.550617</td>\n",
              "      <td>2.512970</td>\n",
              "      <td>2.514840</td>\n",
              "      <td>-1343.934</td>\n",
              "      <td>0.591923</td>\n",
              "      <td>121.1572</td>\n",
              "      <td>58.46031</td>\n",
              "      <td>118.5247</td>\n",
              "      <td>60.9893</td>\n",
              "      <td>0.652618</td>\n",
              "      <td>1.453997</td>\n",
              "      <td>1.811190</td>\n",
              "      <td>0.101106</td>\n",
              "      <td>0.057768</td>\n",
              "      <td>8.076488</td>\n",
              "      <td>0.534679</td>\n",
              "      <td>2.284852</td>\n",
              "      <td>2.286112</td>\n",
              "      <td>-1813.286</td>\n",
              "      <td>0.611738</td>\n",
              "      <td>104.9605</td>\n",
              "      <td>51.84484</td>\n",
              "      <td>106.1112</td>\n",
              "      <td>53.66180</td>\n",
              "      <td>HIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.706301</td>\n",
              "      <td>0.242823</td>\n",
              "      <td>0.001295</td>\n",
              "      <td>0.346778</td>\n",
              "      <td>0.763618</td>\n",
              "      <td>1.259882</td>\n",
              "      <td>0.41684</td>\n",
              "      <td>0.496264</td>\n",
              "      <td>0.913105</td>\n",
              "      <td>0.140831</td>\n",
              "      <td>2.285919</td>\n",
              "      <td>2.145088</td>\n",
              "      <td>0.567527</td>\n",
              "      <td>0.672909</td>\n",
              "      <td>2.555573</td>\n",
              "      <td>0.593553</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.272892</td>\n",
              "      <td>0.511764</td>\n",
              "      <td>0.195769</td>\n",
              "      <td>0.023095</td>\n",
              "      <td>0.002590</td>\n",
              "      <td>0.482297</td>\n",
              "      <td>0.506154</td>\n",
              "      <td>0.004868</td>\n",
              "      <td>-2.146818</td>\n",
              "      <td>1.401919</td>\n",
              "      <td>0.191342</td>\n",
              "      <td>-0.723415</td>\n",
              "      <td>0.286540</td>\n",
              "      <td>4.127019</td>\n",
              "      <td>0.015325</td>\n",
              "      <td>0.202676</td>\n",
              "      <td>0.781621</td>\n",
              "      <td>1.072510</td>\n",
              "      <td>1.530215</td>\n",
              "      <td>0.290888</td>\n",
              "      <td>0.457705</td>\n",
              "      <td>0.748594</td>\n",
              "      <td>...</td>\n",
              "      <td>0.668709</td>\n",
              "      <td>3.024795</td>\n",
              "      <td>3.023263</td>\n",
              "      <td>1975.073</td>\n",
              "      <td>0.580114</td>\n",
              "      <td>124.4475</td>\n",
              "      <td>60.64391</td>\n",
              "      <td>126.9987</td>\n",
              "      <td>61.00551</td>\n",
              "      <td>0.646484</td>\n",
              "      <td>1.617714</td>\n",
              "      <td>2.040294</td>\n",
              "      <td>0.099185</td>\n",
              "      <td>0.057136</td>\n",
              "      <td>7.437950</td>\n",
              "      <td>0.517014</td>\n",
              "      <td>2.466297</td>\n",
              "      <td>2.466509</td>\n",
              "      <td>-11632.240</td>\n",
              "      <td>0.597461</td>\n",
              "      <td>118.1144</td>\n",
              "      <td>62.38587</td>\n",
              "      <td>111.9878</td>\n",
              "      <td>58.4858</td>\n",
              "      <td>0.638641</td>\n",
              "      <td>1.476635</td>\n",
              "      <td>1.970279</td>\n",
              "      <td>0.112537</td>\n",
              "      <td>0.064950</td>\n",
              "      <td>9.006743</td>\n",
              "      <td>0.564074</td>\n",
              "      <td>2.479765</td>\n",
              "      <td>2.478522</td>\n",
              "      <td>1995.745</td>\n",
              "      <td>0.541574</td>\n",
              "      <td>104.6046</td>\n",
              "      <td>51.71428</td>\n",
              "      <td>107.7339</td>\n",
              "      <td>56.51631</td>\n",
              "      <td>KOR</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 6374 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0         1         2         3     ...      6370      6371      6372  6373\n",
              "0  3.530088  0.635352  0.000000  0.641956  ...  51.84484  106.1112  53.66180   HIN\n",
              "1  2.706301  0.242823  0.001295  0.346778  ...  51.71428  107.7339  56.51631   KOR\n",
              "\n",
              "[2 rows x 6374 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIvDtnGIa4md",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "ec2027b0-a481-498a-d8d5-a2a35217956b"
      },
      "source": [
        "y_dev = dev[6373]\n",
        "y_dev.head(2)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    HIN\n",
              "1    KOR\n",
              "Name: 6373, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtF4iFFXbJO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev = dev.iloc[:, :-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odagj-hPcAya",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW6BQgUgNBis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv('csv_test.csv', sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXrLbpHSZcyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "header = np.arange(0, 6374)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8dNyOHvZm1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.columns = header"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl49j4c1ZqYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "4341cd18-2485-4b5f-fc0a-3e16414075b7"
      },
      "source": [
        "test.head(2)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>6334</th>\n",
              "      <th>6335</th>\n",
              "      <th>6336</th>\n",
              "      <th>6337</th>\n",
              "      <th>6338</th>\n",
              "      <th>6339</th>\n",
              "      <th>6340</th>\n",
              "      <th>6341</th>\n",
              "      <th>6342</th>\n",
              "      <th>6343</th>\n",
              "      <th>6344</th>\n",
              "      <th>6345</th>\n",
              "      <th>6346</th>\n",
              "      <th>6347</th>\n",
              "      <th>6348</th>\n",
              "      <th>6349</th>\n",
              "      <th>6350</th>\n",
              "      <th>6351</th>\n",
              "      <th>6352</th>\n",
              "      <th>6353</th>\n",
              "      <th>6354</th>\n",
              "      <th>6355</th>\n",
              "      <th>6356</th>\n",
              "      <th>6357</th>\n",
              "      <th>6358</th>\n",
              "      <th>6359</th>\n",
              "      <th>6360</th>\n",
              "      <th>6361</th>\n",
              "      <th>6362</th>\n",
              "      <th>6363</th>\n",
              "      <th>6364</th>\n",
              "      <th>6365</th>\n",
              "      <th>6366</th>\n",
              "      <th>6367</th>\n",
              "      <th>6368</th>\n",
              "      <th>6369</th>\n",
              "      <th>6370</th>\n",
              "      <th>6371</th>\n",
              "      <th>6372</th>\n",
              "      <th>6373</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.655310</td>\n",
              "      <td>0.707101</td>\n",
              "      <td>0.549536</td>\n",
              "      <td>0.218329</td>\n",
              "      <td>0.397093</td>\n",
              "      <td>0.672678</td>\n",
              "      <td>0.178764</td>\n",
              "      <td>0.275585</td>\n",
              "      <td>0.454349</td>\n",
              "      <td>0.126861</td>\n",
              "      <td>1.637669</td>\n",
              "      <td>1.510808</td>\n",
              "      <td>0.338872</td>\n",
              "      <td>1.608370</td>\n",
              "      <td>7.006693</td>\n",
              "      <td>0.796727</td>\n",
              "      <td>3.31</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.603538</td>\n",
              "      <td>0.188431</td>\n",
              "      <td>0.018131</td>\n",
              "      <td>0.003453</td>\n",
              "      <td>0.000648</td>\n",
              "      <td>0.437392</td>\n",
              "      <td>0.522349</td>\n",
              "      <td>0.004719</td>\n",
              "      <td>-2.008516</td>\n",
              "      <td>1.420886</td>\n",
              "      <td>-0.162471</td>\n",
              "      <td>-0.445818</td>\n",
              "      <td>0.213312</td>\n",
              "      <td>6.395654</td>\n",
              "      <td>0.931578</td>\n",
              "      <td>0.516296</td>\n",
              "      <td>0.774924</td>\n",
              "      <td>1.004606</td>\n",
              "      <td>1.436857</td>\n",
              "      <td>0.229682</td>\n",
              "      <td>0.432251</td>\n",
              "      <td>0.661932</td>\n",
              "      <td>...</td>\n",
              "      <td>0.626472</td>\n",
              "      <td>2.938490</td>\n",
              "      <td>2.935258</td>\n",
              "      <td>909.2727</td>\n",
              "      <td>0.620898</td>\n",
              "      <td>133.9885</td>\n",
              "      <td>65.45541</td>\n",
              "      <td>139.1346</td>\n",
              "      <td>67.50035</td>\n",
              "      <td>0.654187</td>\n",
              "      <td>1.687612</td>\n",
              "      <td>2.188400</td>\n",
              "      <td>0.098632</td>\n",
              "      <td>0.055851</td>\n",
              "      <td>9.069361</td>\n",
              "      <td>0.566309</td>\n",
              "      <td>2.707332</td>\n",
              "      <td>2.704763</td>\n",
              "      <td>1053.936</td>\n",
              "      <td>0.571017</td>\n",
              "      <td>123.4151</td>\n",
              "      <td>58.82057</td>\n",
              "      <td>126.9514</td>\n",
              "      <td>61.27251</td>\n",
              "      <td>0.655597</td>\n",
              "      <td>1.565644</td>\n",
              "      <td>2.010583</td>\n",
              "      <td>0.106984</td>\n",
              "      <td>0.061542</td>\n",
              "      <td>9.450972</td>\n",
              "      <td>0.568960</td>\n",
              "      <td>2.516102</td>\n",
              "      <td>2.515892</td>\n",
              "      <td>11989.970</td>\n",
              "      <td>0.531828</td>\n",
              "      <td>111.1119</td>\n",
              "      <td>56.98126</td>\n",
              "      <td>107.9105</td>\n",
              "      <td>52.70351</td>\n",
              "      <td>TEL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.042234</td>\n",
              "      <td>0.637168</td>\n",
              "      <td>0.833369</td>\n",
              "      <td>0.282849</td>\n",
              "      <td>0.806599</td>\n",
              "      <td>1.476387</td>\n",
              "      <td>0.523750</td>\n",
              "      <td>0.669788</td>\n",
              "      <td>1.193538</td>\n",
              "      <td>0.120878</td>\n",
              "      <td>2.627886</td>\n",
              "      <td>2.507008</td>\n",
              "      <td>0.701294</td>\n",
              "      <td>0.627568</td>\n",
              "      <td>2.386121</td>\n",
              "      <td>0.651618</td>\n",
              "      <td>2.65</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.324752</td>\n",
              "      <td>0.472264</td>\n",
              "      <td>0.194474</td>\n",
              "      <td>0.031513</td>\n",
              "      <td>0.003238</td>\n",
              "      <td>0.446459</td>\n",
              "      <td>0.519110</td>\n",
              "      <td>0.009133</td>\n",
              "      <td>-2.082477</td>\n",
              "      <td>1.346777</td>\n",
              "      <td>0.155630</td>\n",
              "      <td>-0.683619</td>\n",
              "      <td>0.275224</td>\n",
              "      <td>5.894388</td>\n",
              "      <td>0.655731</td>\n",
              "      <td>0.096482</td>\n",
              "      <td>0.748464</td>\n",
              "      <td>1.089888</td>\n",
              "      <td>1.677155</td>\n",
              "      <td>0.341424</td>\n",
              "      <td>0.587267</td>\n",
              "      <td>0.928691</td>\n",
              "      <td>...</td>\n",
              "      <td>0.497612</td>\n",
              "      <td>3.080022</td>\n",
              "      <td>3.077613</td>\n",
              "      <td>1278.5710</td>\n",
              "      <td>0.675890</td>\n",
              "      <td>131.2584</td>\n",
              "      <td>66.83633</td>\n",
              "      <td>133.3082</td>\n",
              "      <td>69.13142</td>\n",
              "      <td>0.653659</td>\n",
              "      <td>1.555861</td>\n",
              "      <td>1.958622</td>\n",
              "      <td>0.099719</td>\n",
              "      <td>0.056963</td>\n",
              "      <td>7.536929</td>\n",
              "      <td>0.534496</td>\n",
              "      <td>2.403392</td>\n",
              "      <td>2.401731</td>\n",
              "      <td>1447.649</td>\n",
              "      <td>0.606914</td>\n",
              "      <td>112.0865</td>\n",
              "      <td>53.31438</td>\n",
              "      <td>111.3412</td>\n",
              "      <td>54.38641</td>\n",
              "      <td>0.645464</td>\n",
              "      <td>1.473443</td>\n",
              "      <td>1.882226</td>\n",
              "      <td>0.104581</td>\n",
              "      <td>0.059532</td>\n",
              "      <td>7.418565</td>\n",
              "      <td>0.517831</td>\n",
              "      <td>2.372760</td>\n",
              "      <td>2.372236</td>\n",
              "      <td>4533.779</td>\n",
              "      <td>0.633764</td>\n",
              "      <td>105.8323</td>\n",
              "      <td>53.93647</td>\n",
              "      <td>101.7573</td>\n",
              "      <td>50.18161</td>\n",
              "      <td>HIN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 6374 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0         1         2         3     ...      6370      6371      6372  6373\n",
              "0  2.655310  0.707101  0.549536  0.218329  ...  56.98126  107.9105  52.70351   TEL\n",
              "1  3.042234  0.637168  0.833369  0.282849  ...  53.93647  101.7573  50.18161   HIN\n",
              "\n",
              "[2 rows x 6374 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExXR_PNaNPMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "20faa8a2-fa1f-47a4-c36a-a86792fb5962"
      },
      "source": [
        "y_test = test[6373]\n",
        "y_test.head(2)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    TEL\n",
              "1    HIN\n",
              "Name: 6373, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWA-SidgXX_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = test.iloc[:, :-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsjMlVOcc03M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "76347766-698f-4e7e-c42b-87c034c4da7a"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      TEL\n",
              "1      HIN\n",
              "2      ITA\n",
              "3      ARA\n",
              "4      ARA\n",
              "      ... \n",
              "861    KOR\n",
              "862    KOR\n",
              "863    GER\n",
              "864    ITA\n",
              "865    ARA\n",
              "Name: 6373, Length: 866, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG8ixXvW3WKl",
        "colab_type": "text"
      },
      "source": [
        "### Encoding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxOjBMrFcVHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWuw_Q3afz87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AssuIth_gFtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_1 = np.array(y_train).reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNmhUTcjf4lU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "00bbce08-8fe3-4161-9d8e-6f4a635227aa"
      },
      "source": [
        "enc.fit(y_train_1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
              "              dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
              "              n_values=None, sparse=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ol3zZkEgkny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_1 = enc.transform(y_train_1).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CikozSF3dir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_dev_1 = np.array(y_dev).reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNWSfPqJ3nMM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a63200cd-e8e1-4b20-c330-5a1daec1d9d1"
      },
      "source": [
        "enc.fit(y_dev_1)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
              "              dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
              "              n_values=None, sparse=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stBpdaVA3rr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_dev_1 = enc.transform(y_dev_1).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XogdmwnoAWtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_1 = np.array(y_test).reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StpP9q2oAjfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "302b1765-0621-45c0-f236-5605180ff157"
      },
      "source": [
        "enc.fit(y_test_1)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
              "              dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
              "              n_values=None, sparse=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRdcEh2PAl5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_1 = enc.transform(y_test_1).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRZ4zgPojn5g",
        "colab_type": "text"
      },
      "source": [
        "## Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT3pq17EjnLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t2647xt8PzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ad5675f-3916-47e9-c32a-1677b18a46e7"
      },
      "source": [
        "!pip install tensorboardcolab"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uazN-_0czxxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e621b066-7f5d-41eb-c4c3-3c6ede825398"
      },
      "source": [
        "print(train.shape)\n",
        "print(y_train.shape)\n",
        "print(dev.shape)\n",
        "print(y_dev.shape)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3299, 6373)\n",
            "(3299,)\n",
            "(964, 6373)\n",
            "(964,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGrjTnn-kBdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = Sequential()\n",
        "network.add(Dense(200, input_dim=6373, activation='sigmoid'))\n",
        "network.add(Dense(150, activation='sigmoid'))\n",
        "#network.add(Dense(100, activation='sigmoid'))\n",
        "#network.add(Dense(100, activation='sigmoid'))\n",
        "network.add(Dense(11, activation='softmax'))\n",
        "\n",
        "network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLv0SOjAj2iD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aca7cadb-6650-4ab9-8f6b-bb9b076a6586"
      },
      "source": [
        "filepath = \"weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "tbc=TensorBoardColab()\n",
        "callbacks_list = [checkpoint, tbc]\n",
        "\n",
        "network.fit(train, y_train_1, epochs=100, batch_size=1000, validation_data=(dev, y_dev_1), callbacks=[checkpoint, TensorBoardColabCallback(tbc)])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://467967cc.ngrok.io\n",
            "Train on 3299 samples, validate on 964 samples\n",
            "Epoch 1/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3042 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.90909, saving model to weights.hdf5\n",
            "Epoch 2/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3042 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.90909\n",
            "Epoch 3/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3042 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.90909\n",
            "Epoch 4/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3041 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.90909\n",
            "Epoch 5/100\n",
            "3299/3299 [==============================] - 0s 106us/step - loss: 0.3043 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.90909\n",
            "Epoch 6/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3041 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.90909\n",
            "Epoch 7/100\n",
            "3299/3299 [==============================] - 0s 102us/step - loss: 0.3041 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.90909\n",
            "Epoch 8/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3041 - acc: 0.9091 - val_loss: 0.3048 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.90909\n",
            "Epoch 9/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3043 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.90909\n",
            "Epoch 10/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3042 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.90909\n",
            "Epoch 11/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3043 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.90909\n",
            "Epoch 12/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3041 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.90909\n",
            "Epoch 13/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3041 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.90909\n",
            "Epoch 14/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.90909\n",
            "Epoch 15/100\n",
            "3299/3299 [==============================] - 0s 103us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.90909\n",
            "Epoch 16/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3041 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.90909\n",
            "Epoch 17/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.90909\n",
            "Epoch 18/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3042 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.90909\n",
            "Epoch 19/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.90909\n",
            "Epoch 20/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.90909\n",
            "Epoch 21/100\n",
            "3299/3299 [==============================] - 0s 104us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.90909\n",
            "Epoch 22/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.90909\n",
            "Epoch 23/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3041 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.90909\n",
            "Epoch 24/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3042 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.90909\n",
            "Epoch 25/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.90909\n",
            "Epoch 26/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3041 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.90909\n",
            "Epoch 27/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.90909\n",
            "Epoch 28/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.90909\n",
            "Epoch 29/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.90909\n",
            "Epoch 30/100\n",
            "3299/3299 [==============================] - 0s 105us/step - loss: 0.3041 - acc: 0.9091 - val_loss: 0.3047 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.90909\n",
            "Epoch 31/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.90909\n",
            "Epoch 32/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.90909\n",
            "Epoch 33/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.90909\n",
            "Epoch 34/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.90909\n",
            "Epoch 35/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.90909\n",
            "Epoch 36/100\n",
            "3299/3299 [==============================] - 0s 101us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.90909\n",
            "Epoch 37/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.90909\n",
            "Epoch 38/100\n",
            "3299/3299 [==============================] - 0s 95us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.90909\n",
            "Epoch 39/100\n",
            "3299/3299 [==============================] - 0s 96us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.90909\n",
            "Epoch 40/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.90909\n",
            "Epoch 41/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.90909\n",
            "Epoch 42/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.90909\n",
            "Epoch 43/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.90909\n",
            "Epoch 44/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.90909\n",
            "Epoch 45/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.90909\n",
            "Epoch 46/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.90909\n",
            "Epoch 47/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.90909\n",
            "Epoch 48/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.90909\n",
            "Epoch 49/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.90909\n",
            "Epoch 50/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.90909\n",
            "Epoch 51/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.90909\n",
            "Epoch 52/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.90909\n",
            "Epoch 53/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.90909\n",
            "Epoch 54/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.90909\n",
            "Epoch 55/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.90909\n",
            "Epoch 56/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.90909\n",
            "Epoch 57/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.90909\n",
            "Epoch 58/100\n",
            "3299/3299 [==============================] - 0s 101us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.90909\n",
            "Epoch 59/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.90909\n",
            "Epoch 60/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.90909\n",
            "Epoch 61/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.90909\n",
            "Epoch 62/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.90909\n",
            "Epoch 63/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.90909\n",
            "Epoch 64/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.90909\n",
            "Epoch 65/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.90909\n",
            "Epoch 66/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.90909\n",
            "Epoch 67/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.90909\n",
            "Epoch 68/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.90909\n",
            "Epoch 69/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.90909\n",
            "Epoch 70/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.90909\n",
            "Epoch 71/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.90909\n",
            "Epoch 72/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.90909\n",
            "Epoch 73/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.90909\n",
            "Epoch 74/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.90909\n",
            "Epoch 75/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.90909\n",
            "Epoch 76/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.90909\n",
            "Epoch 77/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.90909\n",
            "Epoch 78/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.90909\n",
            "Epoch 79/100\n",
            "3299/3299 [==============================] - 0s 101us/step - loss: 0.3036 - acc: 0.9091 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.90909\n",
            "Epoch 80/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.90909\n",
            "Epoch 81/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3047 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.90909\n",
            "Epoch 82/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.90909\n",
            "Epoch 83/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3039 - acc: 0.9091 - val_loss: 0.3042 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.90909\n",
            "Epoch 84/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3038 - acc: 0.9091 - val_loss: 0.3047 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.90909\n",
            "Epoch 85/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3040 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.90909\n",
            "Epoch 86/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.90909\n",
            "Epoch 87/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3039 - acc: 0.9092 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.90909\n",
            "Epoch 88/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3037 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.90909\n",
            "Epoch 89/100\n",
            "3299/3299 [==============================] - 0s 97us/step - loss: 0.3036 - acc: 0.9091 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.90909\n",
            "Epoch 90/100\n",
            "3299/3299 [==============================] - 0s 99us/step - loss: 0.3038 - acc: 0.9092 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.90909\n",
            "Epoch 91/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3036 - acc: 0.9092 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.90909\n",
            "Epoch 92/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3037 - acc: 0.9092 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.90909\n",
            "Epoch 93/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3036 - acc: 0.9091 - val_loss: 0.3045 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.90909\n",
            "Epoch 94/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3036 - acc: 0.9092 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.90909\n",
            "Epoch 95/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3036 - acc: 0.9092 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.90909\n",
            "Epoch 96/100\n",
            "3299/3299 [==============================] - 0s 103us/step - loss: 0.3036 - acc: 0.9092 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.90909\n",
            "Epoch 97/100\n",
            "3299/3299 [==============================] - 0s 101us/step - loss: 0.3037 - acc: 0.9092 - val_loss: 0.3043 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.90909\n",
            "Epoch 98/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3036 - acc: 0.9092 - val_loss: 0.3042 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.90909\n",
            "Epoch 99/100\n",
            "3299/3299 [==============================] - 0s 100us/step - loss: 0.3036 - acc: 0.9092 - val_loss: 0.3046 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.90909\n",
            "Epoch 100/100\n",
            "3299/3299 [==============================] - 0s 98us/step - loss: 0.3036 - acc: 0.9092 - val_loss: 0.3044 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.90909\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fce1e2a7c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22r1j2Gf_-dY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5b92ce72-1fd5-40d4-d8f6-28231ed7cc15"
      },
      "source": [
        "# Evaluate the model.\n",
        "mae, mse = network.evaluate(test, y_test_1)\n",
        "print('MAE: {:.2f}, MSE: {:.2f}'.format(mae, mse))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "866/866 [==============================] - 0s 87us/step\n",
            "MAE: 0.30, MSE: 0.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS8GIsOzBTjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = network.predict(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD6Lg5o1BkCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKdYln6FBcoO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "623ed169-5fa5-42e1-924e-cf0c80ee4999"
      },
      "source": [
        "# Evaluate the quality of the fit.\n",
        "print('R-squared:', r2_score(y_test_1, y_pred))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R-squared: 0.0005870037947847836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBAsCe0gBtAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "12878829-76dc-444f-e313-49064fd7943a"
      },
      "source": [
        "# Plot predictions against ground-truth.\n",
        "plt.xlabel('True values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.axis('square')\n",
        "# Add some padding to the plot so that all the data can fit in well.\n",
        "max_val = 1.1 * max(y_test_1.max(), y_pred.max())\n",
        "plt.xlim([0, max_val])\n",
        "plt.ylim([0, max_val])\n",
        "# Add a diagonal line to appreciate better the predictions.\n",
        "plt.plot([0, max_val], [0, max_val], color='gray')\n",
        "plt.scatter(y_test_1, y_pred)\n",
        "# Save as PNG file and display plot.\n",
        "plt.show()"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAEGCAYAAACQF6v1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmTkYZpaZg2FmmTkYZpaZg2Fmmf0/uD9ObQIoBPsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TX84C3GDWtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}